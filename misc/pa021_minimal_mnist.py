# -*- coding: utf-8 -*-
"""PA021_minimal_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cvAvtd6O2c3SHLpQLLOGfx8kgjxHphrT

# FashionMNIST solutions
"""

from tensorflow.random import set_seed

set_seed(42)  # a truly random seed is an import start

"""## MLP — the expected implementation
> _That's real computation density if I've ever seen one_
— Haskell developer, September 2020

- 6 lines
- 6 epochs
- 6 seconds
- ~88 % test acc
- 6 ECTS

"""

from tensorflow import keras as K

(x_train, y_train), (x_test, y_test) = [(x.reshape((len(x), 784)).astype(float) / 255, K.utils.to_categorical(y)) for
                                        x, y in K.datasets.fashion_mnist.load_data()]

model = K.models.Sequential(
    [K.layers.Dense(256, 'relu', input_shape=(784,)), K.layers.Dropout(0.15), K.layers.Dense(64, 'relu'),
     K.layers.Dense(10, 'softmax')])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=6, batch_size=124, verbose=0, validation_split=0.1)

print(f'Test acc: {model.evaluate(x_test, y_test, verbose=2)[1] * 100:2.4f} %')

"""Runs in 6 seconds on 2 core vCPU as well. GPU computation is not stable == results vary even with the same seed.

Just do it from scratch.

# Other models and their performance

## Random function
> _It ain't much but it's honest work_
— your model without the train method, July 2020
"""

(x_train, y_train), (x_test, y_test) = [(x.reshape((len(x), 784)).astype(float) / 255, K.utils.to_categorical(y)) for
                                        x, y in K.datasets.fashion_mnist.load_data()]

for _ in range(10):
    model = K.models.Sequential([K.layers.Input(shape=(784,)), K.layers.Dense(10, 'softmax')])
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    print(f'Test acc: {model.evaluate(x_test, y_test, verbose=0)[1] * 100:7.4f} %')

"""## Worse than random
> _It ain't much but it's honest work_
— you while debugging it, October 2020
"""

(x_train, y_train), (x_test, y_test) = [(x.reshape((len(x), 784)).astype(float) / 255, K.utils.to_categorical(y)) for
                                        x, y in K.datasets.fashion_mnist.load_data()]


def my_first_loss_attemt(y_true, y_pred):
    return K.losses.categorical_crossentropy(y_true, y_pred - 1)


model = K.models.Sequential([K.layers.Input(shape=(784,)), K.layers.Dense(10, 'softmax')])
model.compile(optimizer='adam', loss=my_first_loss_attemt, metrics=['accuracy'])
model.fit(x_train, y_train, epochs=2, batch_size=256, verbose=2, validation_split=0.1)

print(f'Test acc: {model.evaluate(x_test, y_test, verbose=0)[1] * 100:7.4f} %')

"""## Linear model
> _Nah, that's just 6 lines even in C++_
— random colleague, November 2020
"""

from tensorflow import keras as K

(x_train, y_train), (x_test, y_test) = [(x.reshape((len(x), 784)).astype(float) / 255, K.utils.to_categorical(y)) for
                                        x, y in K.datasets.fashion_mnist.load_data()]

model = K.models.Sequential([K.layers.Input(shape=(784,)), K.layers.Dense(10, 'softmax')])
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(x_train, y_train, epochs=15, batch_size=256, verbose=0, validation_split=0.1)

print(f'Test acc: {model.evaluate(x_test, y_test, verbose=0)[1] * 100:7.4f} %')

"""## CNN
> _A weekend project_
— your teammate, December 2020
"""

from tensorflow import keras as K

(x_train, y_train), (x_test, y_test) = [(x[:, :, :, None].astype(float) / 255, K.utils.to_categorical(y)) for x, y in
                                        K.datasets.fashion_mnist.load_data()]

inputs = K.layers.Input(shape=(28, 28, 1))
x = K.layers.Conv2D(128, 3, activation='relu')(inputs)
for _ in range(3):
    x = K.layers.Conv2D(64, 3, activation='relu')(x)
x = K.layers.MaxPool2D()(x)
x = K.layers.Flatten()(x)
x = K.layers.Dense(16, 'relu')(x)
outputs = K.layers.Dense(10, 'softmax')(x)

model = K.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(x_train, y_train, epochs=7, batch_size=64, verbose=0, validation_split=0.1)

print(f'Test acc: {model.evaluate(x_test, y_test, verbose=0)[1] * 100:7.4f} %')

"""## CNN with extra spice
> _No longer a weekend project even by my standards_
— Ronald Luc, New Year's Eve 2020
"""

from tensorflow import keras as K

(x_train, y_train), (x_test, y_test) = [(x[:, :, :, None].astype(float) / 255, K.utils.to_categorical(y)) for x, y in
                                        K.datasets.fashion_mnist.load_data()]

inputs = K.layers.Input(shape=(28, 28, 1))
x = K.layers.Conv2D(128, 3, activation='relu')(inputs)
for _ in range(3):
    x = K.layers.BatchNormalization()(x)
    x = K.layers.Conv2D(64, 3, activation='relu')(x)
x = K.layers.BatchNormalization()(x)
x = K.layers.MaxPool2D()(x)
x = K.layers.Flatten()(x)
x = K.layers.Dense(16, 'relu')(x)
x = K.layers.BatchNormalization()(x)
outputs = K.layers.Dense(10, 'softmax')(x)

model = K.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
model.fit(x_train, y_train, epochs=7, batch_size=64, verbose=0, validation_split=0.2)

print(f'Test acc: {model.evaluate(x_test, y_test, verbose=0)[1] * 100:7.4f} %')

"""SOTA is 96.91 % acc, but this simple model without hard hyper parameter fine-tuning made it to the top 10 of [Papers with code!](https://paperswithcode.com/sota/image-classification-on-fashion-mnist) Even without any train data augmentation or pre-training.

## Just over the top
> _Not worth the time; unless you wanna achieve SOTA or impress your investor_
— AGI, January 2031
"""

!pip
install
keras - tuner

from tensorflow import keras as K
import kerastuner as kt


def build_model(hp):
    activation = hp.Choice('activation', ['relu', 'elu', ])

    inputs = K.layers.Input(shape=(28, 28, 1))
    x = inputs
    for i in range(hp.Int('cnn_repetitions', 1, 5)):
        x = K.layers.Conv2D(filters=hp.Int(f'cnn_units{i}',
                                           min_value=16,
                                           max_value=256 // min(i + 1, 4),
                                           step=16), kernel_size=3, activation=activation)(x)
        x = K.layers.BatchNormalization()(x)

    x = K.layers.MaxPool2D()(x)
    x = K.layers.Flatten()(x)

    for i in range(hp.Int('dense_repetitions', 0, 2)):
        x = K.layers.Dense(units=hp.Int(f'dnn_units{i}',
                                        min_value=8,
                                        max_value=64,
                                        step=8), activation=activation)(x)
        x = K.layers.BatchNormalization()(x)
    outputs = K.layers.Dense(10, 'softmax')(x)

    model = K.Model(inputs=inputs, outputs=outputs)
    K.optimizers.Adam(hp.Choice('learning_rate',
                                values=[1e-2, 3e-3, 1e-3, 3e-4, 1e-4]))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model


callbacks = [K.callbacks.EarlyStopping(patience=3, restore_best_weights=True), ]

tuner = kt.Hyperband(
    build_model,
    objective='val_accuracy',
    max_epochs=42,
    hyperband_iterations=2,
    directory='my_dir',
    project_name='justnoreasonwhy',
    overwrite=True,
    seed=42
)

(x_train, y_train), (x_test, y_test) = [(x[:, :, :, None].astype(float) / 255, K.utils.to_categorical(y)) for x, y in
                                        K.datasets.fashion_mnist.load_data()]
tuner.search(x_train, y_train, verbose=2,
             validation_split=0.1, callbacks=callbacks)

model = tuner.get_best_models(num_models=1)[0]
print(f'Test acc: {model.evaluate(x_test, y_test, verbose=0)[1] * 100:7.4f} %')

for model in tuner.get_best_models(num_models=5):
    print(f'Test acc: {model.evaluate(x_test, y_test, verbose=0)[1] * 100:7.4f} %')

tuner.get_best_hyperparameters()[0].get_config()['values']

model.summary()

"""Best acc was over 92.4 %, sadly the gColab quotas are now pretty strict and this hyper search could not be finish here."""
